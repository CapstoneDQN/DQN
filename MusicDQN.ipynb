{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf944b5-703f-430b-91a1-473cf0e4c72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# 필요한 라이브러리 임포트\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image\n",
    "import reverb\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment, batched_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "\n",
    "# Music Environment 임포트\n",
    "from music_env import SongCatalog, SongHistoryEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541cb521-16eb-44e3-b1f0-65e363e69a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "print(f\"TensorFlow version: {tf.version.VERSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d3d8f-52cc-4553-8752-9c16d78ad183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "num_iterations = 30000  # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 200000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 5e-4  # @param {type:\"number\"}\n",
    "log_interval = 500  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1500  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6f3e96-1abf-4559-b1db-c70532a6dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "print(\"데이터 로드 중...\")\n",
    "TOP_K = 500\n",
    "df = pd.read_csv('listening_history.csv', sep=r'\\s+', header=None, names=['user', 'song', 'date', 'time'])\n",
    "df['timestamp'] = df['date'] + ' ' + df['time']\n",
    "\n",
    "# 상위 곡 필터링\n",
    "song_counts = df['song'].value_counts()\n",
    "top_items = song_counts.head(TOP_K).index.tolist()\n",
    "df = df[df['song'].isin(top_items)]\n",
    "\n",
    "# 사용자별 시퀀스 생성\n",
    "df = df.sort_values(['user', 'timestamp'])\n",
    "user_groups = df.groupby('user')['song'].apply(list).reset_index()\n",
    "user_groups.columns = ['user_id', 'listening_history']\n",
    "\n",
    "print(f\"총 사용자 수: {len(user_groups)}\")\n",
    "print(f\"상위 곡 수: {len(top_items)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50c688-49b1-452e-a36d-dc885a8c1003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Music Environment 생성\n",
    "print(\"음악 추천 환경 초기화 중...\")\n",
    "song_catalog = SongCatalog(user_groups, top_items)\n",
    "print(\"SVD 모델 훈련 완료\")\n",
    "\n",
    "# 환경 생성\n",
    "train_py_env = SongHistoryEnv(song_catalog, user_groups, max_steps=10)\n",
    "eval_py_env = SongHistoryEnv(song_catalog, user_groups, max_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d08b0-6c29-49b6-8c6e-d1bdb5f35d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# 환경 테스트\n",
    "print('Observation Spec:')\n",
    "print(train_py_env.observation_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abd0ef7-2bbb-42ab-b744-dadc72aea059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "print('Reward Spec:')\n",
    "print(train_py_env.time_step_spec().reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4b167-a2f0-4cba-a954-21fbe3b75fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "print('Action Spec:')\n",
    "print(train_py_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d423a94-f248-4aea-b57b-85266568f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# 환경 동작 확인\n",
    "time_step = train_py_env.reset()\n",
    "print('Time step:')\n",
    "print(time_step)\n",
    "\n",
    "action = np.array(1, dtype=np.int32)\n",
    "next_time_step = train_py_env.step(action)\n",
    "print('Next time step:')\n",
    "print(next_time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c2b32-bf6c-402c-8026-5ac60228e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# TensorFlow 환경으로 래핑\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd23d8-770b-4d3b-8f8a-c11293bbe2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Q-Network 설정\n",
    "fc_layer_params = (64, 32)\n",
    "action_tensor_spec = tensor_spec.from_spec(train_py_env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "print(f\"액션 수: {num_actions}\")\n",
    "\n",
    "# Dense layer 헬퍼 함수 정의\n",
    "def dense_layer(num_units):\n",
    "    return tf.keras.layers.Dense(\n",
    "        num_units,\n",
    "        activation=tf.keras.activations.relu,\n",
    "        kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "            scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# Q-Network 구성\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd3008-55bb-4fb2-9a7b-d796db460c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# DQN 에이전트 생성\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb7a87a-95d3-474b-afa2-388494f4a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# 정책 정의\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203ca1b2-6fa3-4afe-b534-5ae4162397a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# 랜덤 정책 (비교용)\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba54142-390e-4ed8-b3e2-cba0945e2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    total_hits = 0\n",
    "    total_steps = 0\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        episode_hits = 0\n",
    "        episode_steps = 0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            # 핵심 수정 1: Shape 문제 해결\n",
    "            if len(time_step.observation.shape) == 3:\n",
    "                time_step = time_step._replace(\n",
    "                    observation=tf.squeeze(time_step.observation, axis=1)\n",
    "                )\n",
    "            \n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            \n",
    "            # 핵심 수정 2: Format 오류 방지\n",
    "            episode_return += float(time_step.reward)\n",
    "            \n",
    "            # 적중률 계산을 위한 추가\n",
    "            if float(time_step.reward) > 0:\n",
    "                episode_hits += 1\n",
    "            episode_steps += 1\n",
    "            \n",
    "        total_return += episode_return\n",
    "        total_hits += episode_hits\n",
    "        total_steps += episode_steps\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    hit_rate = total_hits / total_steps if total_steps > 0 else 0.0\n",
    "    \n",
    "    return float(avg_return), float(hit_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ea051-4250-4862-99c9-a314d8db506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afad219-df00-4df3-8387-59d48f9473b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Replay Buffer 설정\n",
    "table_name = 'uniform_table'\n",
    "replay_buffer_signature = tensor_spec.from_spec(agent.collect_data_spec)\n",
    "replay_buffer_signature = tensor_spec.add_outer_dim(replay_buffer_signature)\n",
    "\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_max_length,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "    signature=replay_buffer_signature)\n",
    "\n",
    "reverb_server = reverb.Server([table])\n",
    "\n",
    "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    table_name=table_name,\n",
    "    sequence_length=2,\n",
    "    local_server=reverb_server)\n",
    "\n",
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "    replay_buffer.py_client,\n",
    "    table_name,\n",
    "    sequence_length=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b419d964-43a5-4e2a-9e58-1b9db21d3540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# 데이터 수집 스펙 확인\n",
    "print(\"Collect data spec:\")\n",
    "print(agent.collect_data_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_collect_driver = py_driver.PyDriver(\n",
    "    train_py_env,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "        random_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=initial_collect_steps)\n",
    "\n",
    "# 초기 경험 데이터 수집\n",
    "print(\"초기 경험 데이터 수집 중...\")\n",
    "initial_collect_driver.run(train_py_env.reset())\n",
    "print(f\"초기 {initial_collect_steps}개 스텝 수집 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b2b3b8-284b-4287-bc4b-13c6c278979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# 데이터셋 설정\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "print(\"데이터셋 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba9d4a3-7a67-4803-b1b8-a3e30ee13748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# 훈련 시작\n",
    "print(\"DQN 훈련 시작...\")\n",
    "\n",
    "# 최적화를 위한 그래프 함수 래핑\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# 훈련 스텝 카운터 초기화\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# 훈련 전 에이전트 정책 평가\n",
    "print(\"훈련 전 에이전트 평가:\")\n",
    "avg_return, hit_rate = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "print(f\"초기 - 평균 리턴: {avg_return:.3f}, 적중률: {hit_rate:.3f}\")\n",
    "\n",
    "returns = [avg_return]\n",
    "hit_rates = [hit_rate]  \n",
    "\n",
    "\n",
    "# 환경 초기화\n",
    "time_step = train_py_env.reset()\n",
    "\n",
    "# 데이터 수집 드라이버 생성\n",
    "collect_driver = py_driver.PyDriver(\n",
    "    train_py_env,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "        agent.collect_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=collect_steps_per_iteration)\n",
    "\n",
    "# 훈련 루프 - 데이터 수집 및 에이전트 훈련\n",
    "\n",
    "# 손실 값 추적을 위한 리스트 추가\n",
    "losses = []\n",
    "\n",
    "# 훈련 루프에서 손실 값 저장 (위의 훈련 루프 수정)\n",
    "for i in range(num_iterations):\n",
    "    # 한 스텝 데이터 수집 (드라이버 사용)\n",
    "    time_step, _ = collect_driver.run(time_step)\n",
    "    \n",
    "    # 리플레이 버퍼에서 배치 샘플링 및 에이전트 훈련\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "    \n",
    "    step = agent.train_step_counter.numpy()\n",
    "    \n",
    "    # 손실 값 저장\n",
    "    if step % log_interval == 0:\n",
    "        losses.append(float(train_loss))\n",
    "        print(f'스텝 = {step}: 손실 = {train_loss:.3f}')\n",
    "    \n",
    "    # 성능 평가\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return, hit_rate = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print(f'스텝 = {step}: 평균 리턴 = {avg_return:.3f}, 적중률 = {hit_rate:.3f}')\n",
    "        returns.append(avg_return)\n",
    "        hit_rates.append(hit_rate)\n",
    "    \n",
    "    \n",
    "print(\"훈련 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 더 안정적인 성능 평가\n",
    "\n",
    "def stable_evaluation(environment, policy, num_runs=5, episodes_per_run=20):\n",
    "    \"\"\"여러 번 실행해서 평균과 표준편차 계산\"\"\"\n",
    "    all_returns = []\n",
    "    all_hit_rates = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        run_returns = []\n",
    "        run_hits = 0\n",
    "        run_steps = 0\n",
    "        \n",
    "        for episode in range(episodes_per_run):\n",
    "            time_step = environment.reset()\n",
    "            episode_return = 0.0\n",
    "            episode_hits = 0\n",
    "            episode_steps = 0\n",
    "            \n",
    "            while not time_step.is_last():\n",
    "                if len(time_step.observation.shape) == 3:\n",
    "                    time_step = time_step._replace(\n",
    "                        observation=tf.squeeze(time_step.observation, axis=1)\n",
    "                    )\n",
    "                \n",
    "                action_step = policy.action(time_step)\n",
    "                time_step = environment.step(action_step.action)\n",
    "                \n",
    "                episode_return += float(time_step.reward)\n",
    "                if float(time_step.reward) > 0:\n",
    "                    episode_hits += 1\n",
    "                episode_steps += 1\n",
    "                \n",
    "            run_returns.append(episode_return)\n",
    "            run_hits += episode_hits\n",
    "            run_steps += episode_steps\n",
    "        \n",
    "        avg_return = np.mean(run_returns)\n",
    "        hit_rate = run_hits / run_steps if run_steps > 0 else 0\n",
    "        \n",
    "        all_returns.append(avg_return)\n",
    "        all_hit_rates.append(hit_rate)\n",
    "        \n",
    "        print(f\"Run {run+1}: 평균 리턴 = {avg_return:.3f}, 적중률 = {hit_rate:.3f}\")\n",
    "    \n",
    "    final_return = np.mean(all_returns)\n",
    "    return_std = np.std(all_returns)\n",
    "    final_hit_rate = np.mean(all_hit_rates)\n",
    "    hit_rate_std = np.std(all_hit_rates)\n",
    "    \n",
    "    return final_return, return_std, final_hit_rate, hit_rate_std\n",
    "\n",
    "# 안정적인 최종 성능 비교\n",
    "print(\"\\n=== 안정적인 최종 성능 비교 ===\")\n",
    "print(\"랜덤 정책 (5회 평균):\")\n",
    "random_return, random_return_std, random_hit, random_hit_std = stable_evaluation(\n",
    "    eval_env, random_policy, num_runs=5, episodes_per_run=20)\n",
    "print(f\"  평균 리턴: {random_return:.3f} ± {random_return_std:.3f}\")\n",
    "print(f\"  적중률: {random_hit:.3f} ± {random_hit_std:.3f}\")\n",
    "\n",
    "print(\"\\n훈련된 DQN 에이전트 (5회 평균):\")\n",
    "agent_return, agent_return_std, agent_hit, agent_hit_std = stable_evaluation(\n",
    "    eval_env, agent.policy, num_runs=5, episodes_per_run=20)\n",
    "print(f\"  평균 리턴: {agent_return:.3f} ± {agent_return_std:.3f}\")\n",
    "print(f\"  적중률: {agent_hit:.3f} ± {agent_hit_std:.3f}\")\n",
    "\n",
    "improvement_return = agent_return - random_return\n",
    "improvement_hit = agent_hit - random_hit\n",
    "\n",
    "print(f\"\\n개선도:\")\n",
    "print(f\"  리턴 개선: {improvement_return:.3f} ({improvement_return/random_return*100:.1f}%)\")\n",
    "print(f\"  적중률 개선: {improvement_hit:.3f} ({improvement_hit/random_hit*100:.1f}%)\")\n",
    "\n",
    "# 통계적 유의성 검정\n",
    "from scipy import stats\n",
    "\n",
    "def statistical_significance_test(policy1, policy2, environment, num_episodes=50):\n",
    "    \"\"\"두 정책의 성능 차이가 통계적으로 유의한지 검정\"\"\"\n",
    "    returns1 = []\n",
    "    returns2 = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        # Policy 1 평가\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        while not time_step.is_last():\n",
    "            if len(time_step.observation.shape) == 3:\n",
    "                time_step = time_step._replace(\n",
    "                    observation=tf.squeeze(time_step.observation, axis=1)\n",
    "                )\n",
    "            action_step = policy1.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += float(time_step.reward)\n",
    "        returns1.append(episode_return)\n",
    "        \n",
    "        # Policy 2 평가\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        while not time_step.is_last():\n",
    "            if len(time_step.observation.shape) == 3:\n",
    "                time_step = time_step._replace(\n",
    "                    observation=tf.squeeze(time_step.observation, axis=1)\n",
    "                )\n",
    "            action_step = policy2.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += float(time_step.reward)\n",
    "        returns2.append(episode_return)\n",
    "    \n",
    "    # t-검정 수행\n",
    "    t_stat, p_value = stats.ttest_rel(returns2, returns1)  # 순서 주의: agent - random\n",
    "    \n",
    "    print(f\"\\n=== 통계적 유의성 검정 ===\")\n",
    "    print(f\"t-통계량: {t_stat:.3f}\")\n",
    "    print(f\"p-값: {p_value:.6f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"✅ 통계적으로 유의한 개선 (p < 0.05)\")\n",
    "    elif p_value < 0.1:\n",
    "        print(\"⚠️ 경계선상의 개선 (p < 0.1)\")\n",
    "    else:\n",
    "        print(\"❌ 통계적으로 유의하지 않음 (p >= 0.1)\")\n",
    "    \n",
    "    return t_stat, p_value\n",
    "\n",
    "# 통계적 유의성 검정 실행 (선택사항)\n",
    "t_stat, p_value = statistical_significance_test(random_policy, agent.policy, eval_env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
